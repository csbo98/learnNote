# 数学基本概念

## 贝叶斯定理

[贝叶斯定理](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86)。维基百科中对于贝叶斯定理的描述十分精准。

[可以更深入一点的理解贝叶斯定理](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)，这篇博客里面使用Google的单词纠错作为例子讲解了一下贝叶斯的应用。而且比较了贝叶斯方法和最大似然方法的优劣，最大似然方法一个问题是不能够提供决策的全部信息，另一个缺点是不考虑先验概率，即使一个猜测与数据已经非常符合，也不代表这就是一个好的猜测。

[贝叶斯推断](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD)，对贝叶斯应用的一个描述。

## 似然函数

[维基百科似然函数定义，其中列举的例子通俗易懂](https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性（**似然性代表某个参数为特定值的可能性**）。似然函数一个最重要的应用是最大似然估计，即似然函数取得最大值表示相应的参数能够使得统计模型最为合理。最大似然估计的做法为：首先选择似然函数，整理之后求解其最大值点。那么最大似然估计是机器学习中最为常用的参数估计方法之一，机器学习模型的整个建模过程需要一个似然函数来描述在不同模型参数下真实数据发生的概率，找到让真实数据发生概率最大的模型。机器学习求解参数的过程被称为参数估计，机器学习问题也变成求使损失函数最小的最优化问题。

[最大似然估计和最大后验概率估计](https://blog.csdn.net/u011508640/article/details/72815981)

[从概率角度理解线性回归的优化目标](https://zhuanlan.zhihu.com/p/143416436)

## 神经网络

各种神经网络本质上都是一个复杂度的函数，因此只要使用高数中学过的微积分内容就足以应付神经网络中反向传播的求导。

## 联合概率链式法则

P(ABCDE) = P(E|ABCD)P(ABCD) 
         = P(E|ABCD)P(D|ABC)P(ABC)
         = P(E|ABCD)P(D|ABC)P(C|AB)P(AB)
         = P(E|ABCD)P(D|ABC)P(C|AB)P(B|A)P(A)
         = P(A)P(B|A)P(C|AB)P(D|ABC)P(E|ABCD)
[MIT联合概率公开课](https://www.cnblogs.com/azhenwang/p/9174574.html)

