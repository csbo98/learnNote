# 场景面试题

## 大文件排序和去重

我有8G的内存，和一个800TB的文件，每一行会有一行文字；我想在有限的内存资源内对这个800TB的数据文件进行一个去重的排序；怎么做？

面对此类内存远小于文件大小的情况（排序、去重），核心的思路是使用外部排序来解决。首先将大文件切分为若干个内存中能够存储的小文件（切分操作同样要利用内存来实现）；然后对每一个小文件做排序和去重操作；最后一步将所有的小文件合并起来，合并操作可以两个文件一起合并(每一次只需要读出两个文件的第一行，这种操作一定可以)，也可以全部文件放在一起合并(这种情况可以使用优先队列，但是需要考虑内存中能否存下，那么这种情况下可以使用摘要来优化)。

**参考**

1. [Sorting larger file with smaller RAM]([Sorting larger file with smaller RAM - GeeksforGeeks](https://www.geeksforgeeks.org/sorting-larger-file-with-smaller-ram/))
2. [Find duplicates in large file]([algorithm - Find duplicates in large file - Stack Overflow](https://stackoverflow.com/questions/9215820/find-duplicates-in-large-file))
3. [External sorting]([External sorting - Wikipedia](https://en.wikipedia.org/wiki/External_sorting#External_merge_sort))